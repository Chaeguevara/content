---
lastmod: 2023-12-10
---
강화학습과 다른 학습(CNN, NLP 등)과 가장 큰 차이점은 다음키워드로 설명 할 수 있다.
- Evaluate
- Instruct
이 부분에 대한 설명이 약간은 막연하지만 이런 식으로 이해할 수 있다. Evaluate란 선택한 행동이 얼마나 좋은지를 뜻한다. 예를 들면 1~10으로 어떤 행동에 대해 점수를 메기는 행위로 볼 수 있다. 우리가 일상생활에서 하는 대부분이 아마 이렇게 이야기 할 수 있을지도 모른다. 반면 Instruct란 정답이 무엇인지 정해져있는지 그런 이야기다. 가령 지도학습 계열에서 보면 어떤 그림이 강아지, 고양이, 돼지 등등을 분류하는 것은 Instruct에 속한다. 
이 부분이 나도 조금은 잘 이해가 안가긴 한다. 어떤 행동이 얼마나 좋은지를 평가하는 것과 일반적으로 지도학습에서 하는 $J(\theta)$랑 크게 차이가 있나 싶다. 그래서 이부분의 설명은 조금 부족한 점이 있는 것 같고, ==강화학습에서 중요한 점은 한 Agent가 계속해서 Interaction을 한다==는 점. 그래서 같은 행위가 상황에 따라 틀릴 수도 있다는 점이 가장 다른 부분이라고 생각한다.
책의 내용으로 조금 다시 돌아와 보면 K-armed bandit로 간단하게 이해해보기로 한다.

# 1 A K-armed Bandit Problem
미국 말로 ~ problem이라는 말이 나오면 알고리즘이랑 비슷하다고 보면 된다. k-armed bandit라는 말이 다소 생소할 수 있다. 그래서 one-armed bandit 그림을 찾아보면 다음과 같다.
![[Pasted image 20231203204717.png]]


그나마 우리가 아는 슬롯머신이라는 사실을 알 수 있다. 그러면 k-armed bandit라는 것은 팔이 k개 달린 슬롯머신이라는 것을 아 수 있다.

머신을 땡기는 묘미는 ==떙겨서 가장 높은 보상==을 받는 것이다. 가령 총 1000번 당길 수 있을때 가장 높은 기대소득을 만들어 내는 것이다. 1000번 당기는 각 단계를 *time step*이라고 말한다.

용어 측면에서 보면, ==어떤 행위를 했을때 기대 보상 또는 보상의 평균==을 ==*value of the action*==이라고 한다. 그럼 용어를 정리하고 이를 수학 식으로 표현하면 다음과 같다.
- 어떤 단계 time step을 t라 하고
- t에서 선택한 action을 $A_t$
- 이때 따라오는 보상을 $R_t$라 하면
- 어떤 action a에 대한 기대보상(value of action)은
	- $q_{*}(a)\doteq E[R_t|A_t=a]$
이제 강화학습의 어려운 내용이 시작된다. 어떤 행동의기대보상($q_{*}(A)$)을 바로 알기란 사실상 불가능 하다. 그래서 샘플링을 통해 이를 추정하는 방법이 필요하다. t회 반복했을때 value of action의 추정치를 $Q_t(a)$라 하며 우리가 원하는 것은
$q_{*}(a)\doteq E[R_t|A_t=a] = Q_t(a)$라고 정리할 수 있다.

## 1.1 Epsilon greedy
자 여기서 말하고 싶은것은 무엇인가. 비유를 들면 이렇다. 우리가 맛집을 찾을때 항상 검증된 맛집을 가는 방법이 있다. 그러면 항상 어느정도 보장된 맛을 먹을 수 있을 것이다. 하지만 어느날 문득 다른 집을 찾아보지 않는다면 우리는 계속 그 맛집의 맛만 보게 된다. 이 비유에서 현재 가장 좋은 맛을 가진 집을 가는 것을 Exploitation이라 한다. 반면 새로운 집을 탐험하는 것을 exploration이라고 한다. 
Greedy action이란 현재를 기준으로 가장 좋은것만, 다시말하면 exploitation만 계속 하는 상태를 뜻한다. 하지만 위 비유에서 추정할 수 있듯, greedy action은 global optimum한 값을 보장하지 않는다. 그래서 exploration이 필요하다. $\epsilon$ 을 기준으로 exploration과 exploitation을 와리가리 하는 것을 $\epsilon-greedy$라고 한다. 

# 2 Action value 
action-value methods란 무엇이냐. 1. action의 value를 추정하는  방법 2. 추정치를 가지고 action을 선택할 때 쓰는 것. 이 둘을 뜻한다. 이를 수학 식으로 하면 다음과 같다. 한국말로 써볼까?
$$Q_{t}(a) \doteq \frac{\text{t이전에 a를 선택해 받은 보상의 합} }{\text{t이전에 a가 선택된 횟수}} = \frac{\sum_{i=1}^{t-1}R_i\cdot\mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb{1}_{A_i=a}}$$
쉽게 말하면 각 action a에 대한 리워드의 평균값. 이게 끝인데. ==근데 여기서 $\mathbb{1}$이 나오는 이유는 어떤 시점 까지 action이 중구난방 선택==되었을 것이다. 그렇기 때문에 전체 스토리 중 우리가 원하는 그 액션이 선택된 횟수와 그때의 reward의 합이 필요하다. `numpy`나 `pandas`로 얘기하면 전체 a==rray에 대한 boolean mask 정도==로 보면 될  것 같다.

greedy action selection이란 각 time step에서 가장 큰 보상을 주는 액션을 선택하는 것이다. [[#1.1 Epsilon greedy]]파트에서 맛집을 선택하는 예를 들었다. ==한 시점에서 가장 맛있는 집을 계속 선택하는 것을 greedy action selection이라 한다.== 이를 좀더 있어보이게 표현하면 아래와 같다.
$$A_t = argmax_{a}Q(a)$$
머신러닝 수업을 들었으면 좀 알겠지만 그래도 내가 설명해본다. Q(a)값을 최대로 하는 action a를 선택하고 이를 A_t에 할당한다는 뜻이다. 